{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import concatenate, Activation, GlobalAveragePooling1D, GlobalMaxPooling1D, Layer, Dense, Embedding, LSTM, GRU, Dropout, SpatialDropout1D, Input, Average, Bidirectional, BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't hog GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training and testing data file. This data can be downloaded from a link, details of which will be provided.\n",
    "trainDataPath = \"./train.txt\"\n",
    "testDataPath = \"./dev.txt\"\n",
    "# Output file that will be generated. This file can be directly submitted.\n",
    "solutionPath = \"./test.txt\"\n",
    "# Path to directory where GloVe file is saved.\n",
    "gloveDir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4                 # Number of classes - Happy, Sad, Angry, Others\n",
    "MAX_NB_WORDS = 7500                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer \n",
    "MAX_SEQUENCE_LENGTH = 30         # All sentences having lesser number of words than this will be padded\n",
    "EMBEDDING_DIM = 300               # The dimension of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/paragag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def cleanText(text, remEmojis=True, meta=True, abbrfix=True):\n",
    "    if abbrfix:\n",
    "        text = re.sub(r\"\\b[u]+\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bm\\b\", \"am\", text)\n",
    "        text = re.sub(r\"\\bn\\b\", \"and\", text)\n",
    "        text = re.sub(r\"\\bluv\\b\", \"love\", text)\n",
    "        text = re.sub(r\"\\bans\\b\", \"answer\", text)\n",
    "        text = re.sub(r\"\\bwt\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\br\\b\", \"are\", text)\n",
    "        text = re.sub(r\"\\bur\\b\", \"your\", text)\n",
    "        text = re.sub(r\"\\bnthng\\b\", \"nothing\", text)\n",
    "        text = re.sub(r\"\\btxt\\b\", \"text\", text)\n",
    "        text = re.sub(r\"\\bconvo\\b\", \"conversation\", text)\n",
    "        text = re.sub(r\"\\bdont\\b\", \"do not\", text)\n",
    "        text = re.sub(r\"\\bim\\b\", \"i am\", text)\n",
    "        text = re.sub(r\"\\bi m\\b\", \"i am\", text)\n",
    "        text = re.sub(r\"\\bwlcm\\b\", \"welcome\", text)\n",
    "        text = re.sub(r\"\\bdnt\\b\", \"did not\", text)\n",
    "        text = re.sub(r\"\\bknw\\b\", \"know\", text)\n",
    "        text = re.sub(r\"\\bsry\\b\", \"sorry\", text)\n",
    "        text = re.sub(r\"\\bchating\\b\", \"chatting\", text)\n",
    "        text = re.sub(r\"\\bfrnds\\b\", \"friends\", text)\n",
    "        text = re.sub(r\"\\bsrry\\b\", \"sorry\", text)\n",
    "        text = re.sub(r\"\\burself\\b\", \"yourself\", text)\n",
    "        text = re.sub(r\"&amp;\", \"and\", text)\n",
    "        text = re.sub(r\"&apos;\", \"\", text)\n",
    "        text = re.sub(r\"\\btal\\b\", \"talk\", text)\n",
    "        text = re.sub(r\"\\bsec\\b\", \"second\", text)\n",
    "        text = re.sub(r\"\\bmin\\b\", \"minute\", text)\n",
    "        text = re.sub(r\"\\bfr\\b\", \"for\", text)\n",
    "        text = re.sub(r\"\\bwrk\\b\", \"work\", text)\n",
    "        text = re.sub(r\"\\bfrm\\b\", \"from\", text)\n",
    "        text = re.sub(r\"\\bwr are\\b\", \"where are\", text)\n",
    "        text = re.sub(r\"\\bwrkng\\b\", \"working\", text)\n",
    "        text = re.sub(r\"\\bmyslf\\b\", \"myself\", text)\n",
    "        text = re.sub(r\"\\bbtr\\b\", \"better\", text)\n",
    "        text = re.sub(r\"\\bdon't\\b\", \"do not\", text)\n",
    "        text = re.sub(r\"\\bdon’t\\b\", \"do not\", text)\n",
    "        text = re.sub(r\"\\bi'm\\b\", \"i am\", text)\n",
    "        text = re.sub(r\"\\bit's\\b\", \"it is\", text)\n",
    "        text = re.sub(r\"\\byou're\\b\", \"you are\", text)\n",
    "        text = re.sub(r\"\\byou’re\\b\", \"you are\", text)\n",
    "        text = re.sub(r\"\\bthat's\\b\", \"that is\", text)\n",
    "        text = re.sub(r\"\\bcan't\\b\", \"cannot\", text)\n",
    "        text = re.sub(r\"\\bcan’t\\b\", \"cannot\", text)\n",
    "        text = re.sub(r\"\\bwhat's\\b\", \"what is\", text)\n",
    "        text = re.sub(r\"\\bwhat’s\\b\", \"what is\", text)\n",
    "        text = re.sub(r\"\\bdidn't\\b\", \"did not\", text)\n",
    "        text = re.sub(r\"\\bi'll\\b\", \"i will\", text)\n",
    "        text = re.sub(r\"\\blet's\\b\", \"let us\", text)\n",
    "        text = re.sub(r\"\\bi've\\b\", \"i have\", text)\n",
    "        text = re.sub(r\"\\bwon't\\b\", \"will not\", text)\n",
    "        text = re.sub(r\"\\bdoesn't\\b\", \"does not\", text)\n",
    "        text = re.sub(r\"\\bit'll\\b\", \"it will\", text)\n",
    "        text = re.sub(r\"\\bofcourse\\b\", \"of course\", text)\n",
    "        text = re.sub(r\"\\bbc[u]?z\\b\", \"because\", text)\n",
    "        text = re.sub(r\"\\bwe'll\\b\", \"we will\", text)\n",
    "        text = re.sub(r\"\\bwhen's\\b\", \"when is\", text)\n",
    "        text = re.sub(r\"\\bwe've\\b\", \"we have\", text)\n",
    "        text = re.sub(r\"\\bhe's\\b\", \"he is\", text)\n",
    "        text = re.sub(r\"\\bshe's\\b\", \"she is\", text)\n",
    "        text = re.sub(r\"\\bfrnd\\b\", \"friend\", text)\n",
    "        text = re.sub(r\"\\bi’m\\b\", \"i am\", text)\n",
    "        text = re.sub(r\"\\bthnks\\b\", \"thanks\", text)\n",
    "        text = re.sub(r\"\\bi’am\\b\", \"i am\", text)\n",
    "        text = re.sub(r\"\\bisn't\\b\", \"is not\", text)\n",
    "        text = re.sub(r\"\\bhaven't\\b\", \"have not\", text)\n",
    "        text = re.sub(r\"\\bhow's\\b\", \"how is\", text)\n",
    "        text = re.sub(r\"\\bhow're\\b\", \"how are\", text)\n",
    "        text = re.sub(r\"\\bhowz\\b\", \"how is\", text)\n",
    "        text = re.sub(r\"\\bwasn't\\b\", \"was not\", text)\n",
    "        text = re.sub(r\"\\bthere's\\b\", \"there is\", text)\n",
    "        text = re.sub(r\"\\bwe're\\b\", \"we are\", text)\n",
    "        text = re.sub(r\"\\byou'll\\b\", \"you will\", text)\n",
    "        text = re.sub(r\"\\bcouldn't\\b\", \"could not\", text)\n",
    "        text = re.sub(r\"\\bthey're\\b\", \"they are\", text)\n",
    "        text = re.sub(r\"\\bit’s\\b\", \"it is\", text)\n",
    "        text = re.sub(r\"\\bthat’s\\b\", \"that is\", text)\n",
    "        text = re.sub(r\"\\bain't\\b\", \"is not\", text)\n",
    "        text = re.sub(r\"\\bwho's\\b\", \"who is\", text)\n",
    "        text = re.sub(r\"\\byou've\\b\", \"you have\", text)\n",
    "        text = re.sub(r\"\\bwhere's\\b\", \"where is\", text)\n",
    "        text = re.sub(r\"\\bshouldn't\\b\", \"should not\", text)\n",
    "        text = re.sub(r\"\\bwouldn't\\b\", \"would not\", text)\n",
    "        text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bit'd\\b\", \"it would\", text)\n",
    "        text = re.sub(r\"\\bidk\\b\", \"i do not know\", text)\n",
    "        text = re.sub(r\"\\brofl\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\blmao\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\bweren't\\b\", \"were not\", text)\n",
    "        text = re.sub(r\"\\bone's\\b\", \"one is\", text)\n",
    "        text = re.sub(r\"\\bwhay\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\bsomthing\\b\", \"something\", text)\n",
    "        text = re.sub(r\"\\bhow've\\b\", \"how have\", text)\n",
    "        text = re.sub(r\"\\bhurted\\b\", \"hurt\", text)\n",
    "        text = re.sub(r\"\\bshutup\\b\", \"shut up\", text)\n",
    "        text = re.sub(r\"\\bwhatsup\\b\", \"what is up\", text)\n",
    "        text = re.sub(r\"\\bintrested\\b\", \"interested\", text)\n",
    "        text = re.sub(r\"\\btbh\\b\", \"to be honest\", text)\n",
    "        text = re.sub(r\"\\btmrw\\b\", \"tomorrow\", text)\n",
    "        text = re.sub(r\"\\bopps\\b\", \"oops\", text)\n",
    "        text = re.sub(r\"\\balrighty\\b\", \"alright\", text)\n",
    "        text = re.sub(r\"\\bdumbass\\b\", \"dumb ass\", text)\n",
    "        text = re.sub(r\"\\bfrds\\b\", \"friends\", text)\n",
    "        text = re.sub(r\"\\bcomeon\\b\", \"come on\", text)\n",
    "        text = re.sub(r\"\\bhogaya\\b\", \"done\", text)\n",
    "        text = re.sub(r\"\\bfrst\\b\", \"first\", text)\n",
    "        text = re.sub(r\"\\bmsgs\\b\", \"messages\", text)\n",
    "        text = re.sub(r\"\\bttyl\\b\", \"talk to you later\", text)\n",
    "        text = re.sub(r\"\\bthts\\b\", \"that is\", text)\n",
    "        text = re.sub(r\"\\bikr\\b\", \"i know right ?\", text)\n",
    "        text = re.sub(r\"\\bthanku\\b\", \"thank you\", text)\n",
    "        text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bwhts\\b\", \"what is\", text)\n",
    "        text = re.sub(r\"\\byou'are\\b\", \"you are\", text)\n",
    "        text = re.sub(r\"\\bypu\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bryt\\b\", \"right\", text)\n",
    "        text = re.sub(r\"\\banytym\\b\", \"anytime\", text)\n",
    "        text = re.sub(r\"\\bbitch(.*)\\b\", \"bitch\", text)\n",
    "        text = re.sub(r\"\\blyk\\b\", \"like\", text)\n",
    "        text = re.sub(r\"\\bgf\\b\", \"girlfriend\", text)\n",
    "        text = re.sub(r\"\\bbf\\b\", \"boyfriend\", text)\n",
    "        text = re.sub(r\"\\bnopes\\b\", \"nope\", text)\n",
    "        text = re.sub(r\"\\behat\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\babt\\b\", \"about\", text)\n",
    "        text = re.sub(r\"\\bhppnd\\b\", \"happened\", text)\n",
    "        text = re.sub(r\"\\bbtwn\\b\", \"between\", text)\n",
    "        text = re.sub(r\"\\bsended\\b\", \"sent\", text)\n",
    "        text = re.sub(r\"\\bmrng\\b\", \"morning\", text)\n",
    "        text = re.sub(r\"\\biwant\\b\", \"I want\", text)\n",
    "        text = re.sub(r\"\\bprblm\\b\", \"problem\", text)\n",
    "        text = re.sub(r\"\\btomorow\\b\", \"tomorrow\", text)\n",
    "        text = re.sub(r\"\\bahaha\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\bo[h]+[k]+\\b\", \"okay\", text)\n",
    "        text = re.sub(r\"\\byour's\\b\", \"yours\", text)\n",
    "        text = re.sub(r\"\\bnvm\\b\", \"never mind\", text)\n",
    "        text = re.sub(r\"\\btlk\\b\", \"talk\", text)\n",
    "        text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bbbey\\b\", \"baby\", text)\n",
    "        text = re.sub(r\"\\bgril\\b\", \"grill\", text)\n",
    "        text = re.sub(r\"\\broboy\\b\", \"robot\", text)\n",
    "        text = re.sub(r\"\\b[']?i\\b\", \"I\", text)\n",
    "        text = re.sub(r\"\\bintrest\\b\", \"interest\", text)\n",
    "        text = re.sub(r\"\\blonley\\b\", \"lonely\", text)\n",
    "        text = re.sub(r\"\\b-\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\babt\\b\", \"about\", text)\n",
    "        text = re.sub(r\"\\bbday\\b\", \"birthday\", text)\n",
    "        text = re.sub(r\"\\bohkay\\b\", \"okay\", text)\n",
    "        text = re.sub(r\"\\bne\\b\", \"me\", text)\n",
    "        text = re.sub(r\"\\bwat\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\bwatever\\b\", \"whatever\", text)\n",
    "        text = re.sub(r\"\\brply\\b\", \"reply\", text)\n",
    "        text = re.sub(r\"\\btnx\\b\", \"thanks\", text)\n",
    "        text = re.sub(r\"\\bawsm\\b\", \"awesome\", text)\n",
    "        text = re.sub(r\"\\bcr[a]+[z]+y\\b\", \"crazy\", text)\n",
    "        text = re.sub(r\"\\bdidnt\\b\", \"did not\", text)\n",
    "        text = re.sub(r\"\\bdonot\\b\", \"do not\", text)\n",
    "        text = re.sub(r\"\\bbestfriend\\b\", \"best friend\", text)\n",
    "        text = re.sub(r\"\\bsmthing\\b\", \"something\", text)\n",
    "        text = re.sub(r\"\\bw[a]+[t]+\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\bwh[a]+t\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\b[y]+[a]+[y]+\\b\", \"yay\", text)\n",
    "        text = re.sub(r\"\\ber[r]+\\b\", \"err\", text)\n",
    "        text = re.sub(r\"\\btel[l]+\\b\", \"tell\", text)\n",
    "\n",
    "    \n",
    "    emojis = ['😂', '😭', '😞', '😢', '😁', '😅', '😍',\n",
    "              '😀', '😃', '😡', '😄', '😆', '😒', '😊',\n",
    "              '😌', '😠', '😤', '🙂', '😺', '😫', '😩',\n",
    "              '😹', '😜', '👍', '😘', '😸', '😉', '😽',\n",
    "              '😻', '😏', '💔', '😝', '😑', '🙁', '😾',\n",
    "              '😿', '😬', '❤', '😋', '🙄', '😔', '🙀',\n",
    "              '😎', '👎', '😦', '😧', '❤️', '😛', '😶',\n",
    "              '😐', '👌', '🤔','😇', '😨', '😯', '😳',\n",
    "              '☹️', '💋', '👋', '😪', '😥', '💕', '😱',\n",
    "              '🙈', '😟', '🙏', '✌', '😖', '😣', '😮',\n",
    "              '🤗', '😓', '😷', '☹', '💞', '🏻', '🙌',\n",
    "              '💐', '🙊', '😰', '☺', '😴', '🖕', '♥', '😕',\n",
    "              '😈', '👿','💗', '♡', '👀', '👊', '‑c', '🖑',\n",
    "              ' 8‑d', ' ‑d', '👻', '：）', '.', '?', '!', ',',\n",
    "              '-', '・', \"'-'\", '\\U0001f923','・ω・', '\\U000fe339', ' ‑c', '_']\n",
    "    \n",
    "    if remEmojis:\n",
    "        for emoji in emojis:\n",
    "            text = text.replace(emoji, \"\")\n",
    "    \n",
    "    if meta:\n",
    "        text = re.sub(r\"\\b[y]+\\b\", \"why\", text)\n",
    "        text = re.sub(r\"\\bpl[s]+\\b\", \"please\", text)\n",
    "        text = re.sub(r\"\\bpl[z]+\\b\", \"please\", text)\n",
    "        text = re.sub(r\"\\baren't\\b\", \"are not\", text)\n",
    "        text = re.sub(r\"\\bb[e]?coz\\b\", \"because\", text)\n",
    "        text = re.sub(r\"\\by[a]+r\\b\", \"yaar\", text)\n",
    "        text = re.sub(r\"\\bth[a]?nx\\b\", \"thanks\", text)\n",
    "        text = re.sub(r\"\\bye[s]+\\b\", \"yes\", text)\n",
    "        text = re.sub(r\"\\b[o]+[k]+\\b\", \"okay\", text)\n",
    "        text = re.sub(r\"\\bha[ha]+\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\bhe[he]+\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\bby[y]+\\b\", \"bye\", text)\n",
    "        text = re.sub(r\"\\b[l]+[o]+[l]+\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\bum[m]+\\b\", \"umm\", text)\n",
    "        text = re.sub(r\"\\bho[o]+\\b\", \"who\", text)\n",
    "        text = re.sub(r\"\\bhe[y]+\\b\", \"hey\", text)\n",
    "        text = re.sub(r\"\\bkn[o]+[w]+\\b\", \"know\", text)\n",
    "        text = re.sub(r\"\\byu[p]+\\b\", \"yup\", text)\n",
    "        text = re.sub(r\"\\bhref\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\byo[u]+\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bye[a]+[h]*\\b\", \"yeah\", text)\n",
    "        text = re.sub(r\"\\byo[u]+\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bx‑d\\b\", \"haha\", text)\n",
    "        text = re.sub(r\"\\bna[h]+\\b\", \"nah\", text)\n",
    "        text = re.sub(r\"\\bto[o]+\\b\", \"too\", text)\n",
    "        text = re.sub(r\"\\b'i\\b\", \"i\", text)\n",
    "        text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "        text = re.sub(r\"\\bnt[n]?g\\b\", \"nothing\", text)\n",
    "        text = re.sub(r\"\\bi\\b\", \"I\", text)\n",
    "        text = re.sub(r\"\\b[m]+[e]+\\b\", \"me\", text)\n",
    "        text = re.sub(r\"\\b[o]+[k]+[a]+[y]+\\b\", \"okay\", text)\n",
    "        text = re.sub(r\"\\b[o]+[h]+\\b\", \"oh\", text)\n",
    "        text = re.sub(r\"\\b[b]+[y]+[e]+\\b\", \"bye\", text)\n",
    "        text = re.sub(r\"\\by[a]+\\b\", \"ya\", text)\n",
    "        text = re.sub(r\"\\b[w]+[h]+[y]+\\b\", \"why\", text)\n",
    "        text = re.sub(r\"\\b[w]+[o]+[w]+\\b\", \"wow\", text)\n",
    "        text = re.sub(r\"\\b[a]+[w]+\\b\", \"aww\", text)\n",
    "        text = re.sub(r\"\\b[h]+[m]+\\b\", \"hmm\", text)\n",
    "        text = re.sub(r\"\\bwh[a]+\\b\", \"what\", text)\n",
    "        text = re.sub(r\"\\bn[a]+[h]*\\b\", \"no\", text)\n",
    "        text = re.sub(r\"\\bncr[a]+[z]+[y]+\\b\", \"crazy\", text)\n",
    "        text = re.sub(r\"\\b[y]+[a]*[r]+\\b\", \"friend\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        raw_conversations : All conversations together\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    raw_conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            raw_conv = ' '.join(line[:].strip().split('\\t')[1:4])\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            processedData = cleanText(conv.lower())\n",
    "            \n",
    "            processedData = re.sub(r\"\\bi\\b\", \"I\", processedData)\n",
    "            conversations.append(processedData)\n",
    "            raw_conversations.append(raw_conv)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, raw_conversations, labels\n",
    "    else:\n",
    "        return indices, conversations, raw_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    with io.open(os.path.join(gloveDir, 'glove.6B.%dd.txt' % EMBEDDING_DIM), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    oov = []\n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    count, total= 0, 0\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        total += 1\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "            count += 1\n",
    "        else:\n",
    "            oov.append(word)\n",
    "    \n",
    "    print(\"Found embedding for\", str((100 * count) / total), \"% embeddings\")\n",
    "    return embeddingMatrix, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGoogleEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    from gensim.models import KeyedVectors as kv\n",
    "    embeddingsIndex = kv.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "    oov = []\n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    count, total= 0, 0\n",
    "    for word, i in wordIndex.items():\n",
    "        total += 1\n",
    "        try:\n",
    "            embeddingVector = embeddingsIndex[word]\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "            count += 1\n",
    "        except:\n",
    "            oov.append(word)\n",
    "    \n",
    "    print(\"Found embedding for\", str((100 * count) / total), \"% embeddings\")\n",
    "    return embeddingMatrix, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googleEmbeddingMatrix, googleOov = getGoogleEmbeddingMatrix(wordIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Extracting tokens...\n",
      "Found 14670 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, trainTexts, rawtrainTexts, labels = preprocessData(trainDataPath, mode=\"train\")\n",
    "labels = to_categorical(np.asarray(labels), NUM_CLASSES)\n",
    "print(\"Processing test data...\")\n",
    "testIndices, testTexts, rawtestTexts, testLabels = preprocessData(testDataPath, mode=\"train\")\n",
    "testLabels = to_categorical(np.asarray(testLabels), NUM_CLASSES)\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(trainTexts)\n",
    "trainSequences = tokenizer.texts_to_sequences(trainTexts)\n",
    "testSequences = tokenizer.texts_to_sequences(testTexts)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage (cutoff length): 0.978945623342175\n"
     ]
    }
   ],
   "source": [
    "lens = [len(x) for x in trainSequences]\n",
    "print(\"Coverage (cutoff length):\", np.sum(np.array(lens) <= MAX_SEQUENCE_LENGTH) / len(trainSequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage with 7500 words: 0.9847698268390912\n"
     ]
    }
   ],
   "source": [
    "sorted_wordcounts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "words_covered, total_words = 0, 0\n",
    "for i, tup in enumerate(sorted_wordcounts):\n",
    "    total_words += tup[1]\n",
    "    if i < MAX_NB_WORDS:\n",
    "        words_covered += tup[1]\n",
    "print(\"Coverage with %d words:\" % MAX_NB_WORDS, words_covered/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_three(texts, tknzr):\n",
    "    middle, left, right = [], [], []\n",
    "    for text in texts:\n",
    "        l, m, r = text.split(' <eos> ')\n",
    "        middle.append(m)\n",
    "        left.append(l)\n",
    "        right.append(r)\n",
    "    tokenize = lambda x: tknzr.texts_to_sequences(x)\n",
    "    return (tokenize(left), tokenize(middle), tokenize(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l, train_m, train_r = split_into_three(trainTexts, tokenizer)\n",
    "test_l, test_m, test_r = split_into_three(testTexts, tokenizer)\n",
    "\n",
    "train_all = tokenizer.texts_to_sequences(trainTexts)\n",
    "test_all = tokenizer.texts_to_sequences(testTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not worry I am girl <eos> hmm how do I know if you are <eos> what is your name  \n",
      "do not worry  i am girl hmm how do I know if you are What's your name?\n",
      "\n",
      "when did I  <eos> saw many times I think  <eos> no  I never saw you\n",
      "When did I? saw many times i think -_- No. I never saw you\n",
      "\n",
      "by <eos> by google chrome <eos> where you live\n",
      "By by Google Chrome Where you live\n",
      "\n",
      "you are ridiculous <eos> I might be ridiculous but I am telling the truth  <eos> you little disgusting whore\n",
      "you are ridiculous I might be ridiculous but I am telling the truth. you little disgusting whore\n",
      "\n",
      "just for time pass <eos> what do you do for a living then <eos> maybe\n",
      "Just for time pass wt do you do for a living then Maybe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(trainTexts[i])\n",
    "    print(rawtrainTexts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Found 400000 word vectors.\n",
      "Found embedding for 73.653715064758 % embeddings\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix, oov = getEmbeddingMatrix(wordIndex)\n",
    "oov = [(x, tokenizer.word_counts.get(x, 0)) for x in oov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"i'd\", 82), ('‑', 54), ('️', 38), ('emoji', 31), ('‑d', 23), (\"'i\", 23), (\"you'd\", 21), (\"'‑\", 20), ('chatbot', 18), ('selfie', 17), (\"'you\", 16), ('i̇', 15), (\"someone's\", 11), (\"valentine's\", 10), (\"friend's\", 10), ('○', 9), ('️️', 9), (\"life's\", 8), ('emojis', 8), (\"today's\", 8), ('friendzone', 8), ('hmmzoning', 8), ('oho', 8), ('gng', 7), (\"'o\", 7), ('tajmahal', 7), ('i’ve', 7), ('‑3', 7), ('bangaram', 7), ('bestie', 6), (\"people's\", 6), ('ehh', 6), ('nthg', 6), (\"here's\", 6), ('nonveg', 6), ('nthing', 6), (\"could've\", 6), ('friendzoned', 6), (\"d‑'\", 6), ('rted', 6), ('let’s', 6), ('oky', 5), (\"it'\", 5), (\"nothing's\", 5), ('ilove', 5), ('cleverbot', 5), ('murgi', 5), (\"everyone's\", 5), ('selfies', 5), ('i’ll', 5), ('beby', 5), ('isn’t', 5), ('oww', 4), ('gtg', 4), ('facepalm', 4), (\"what'\", 4), (\"doin'\", 4), (\"everything's\", 4), ('favs', 4), (\"mother's\", 4), ('habbit', 4), ('srsly', 4), ('nambar', 4), ('whre', 4), (\"wht's\", 4), ('�', 4), ('wiil', 4), ('lolz', 4), ('probs', 4), (\"man's\", 4), (\"why're\", 4), ('whatsaap', 4), ('freecharge', 4), (\"mom's\", 4), ('cmon', 4), ('vry', 4), ('vidio', 4), (\"'just\", 4), ('happies', 4), ('rommate', 4), ('ddlj', 4), ('didn’t', 4), ('brokeup', 4), (\"now'\", 3), ('thnk', 3), ('bhindi', 3), ('frienda', 3), ('how’s', 3), ('hinglish', 3), (\"'love'\", 3), (\"i'and\", 3), ('useropenreflink', 3), ('everythng', 3), (\"'that\", 3), ('lols', 3), ('⚽', 3), ('evrything', 3), (\"mum's\", 3), (\"how'd\", 3), ('hyd', 3), ('wrking', 3), ('seee', 3), ('tshirts', 3), ('😗', 3), ('wouldn’t', 3), ('🌍', 3), (\"haha'\", 3), ('frinds', 3), ('diffrent', 3), (\"you'\", 3), ('achha', 3), ('awsome', 3), ('whatt', 3), (\"she'll\", 3), ('iwill', 3), ('awwh', 3), ('welll', 3), (\"not'\", 3), ('ohhhooo', 3), (\"tonight's\", 3)]\n"
     ]
    }
   ],
   "source": [
    "print(oov[:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googleOov = [(x, tokenizer.word_counts.get(x, 0)) for x in googleOov]\n",
    "googleOov.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(googleOov[:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution file...\n",
      "Shape of training data tensor:  (30160, 30)\n",
      "Shape of label tensor:  (30160, 4)\n"
     ]
    }
   ],
   "source": [
    "train_l = pad_sequences(train_l, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_m = pad_sequences(train_m, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_r = pad_sequences(train_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_all = pad_sequences(train_all, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Creating solution file...\")\n",
    "test_l = pad_sequences(test_l, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_m = pad_sequences(test_m, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_r = pad_sequences(test_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_all = pad_sequences(test_all, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Shape of training data tensor: \", train_l.shape)\n",
    "print(\"Shape of label tensor: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1), NUM_CLASSES)\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def microF1Loss(ground, predictions):\n",
    "    discretePredictions = K.one_hot(K.argmax(predictions, axis=1), NUM_CLASSES)\n",
    "    \n",
    "    truePositives = K.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = K.sum(K.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = K.sum(K.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall + K.epsilon())\n",
    "        \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall + K.epsilon())\n",
    "    \n",
    "    truePositives = K.sum(truePositives[1:])\n",
    "    falsePositives = K.sum(falsePositives[1:])\n",
    "    falseNegatives = K.sum(falseNegatives[1:])\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall + + K.epsilon())\n",
    "    \n",
    "    return microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSingleModel(embeddingMatrix, hidDim=32, learnEmbs=False):\n",
    "    if learnEmbs:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH)\n",
    "    else:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embeddingMatrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embeddingLayer(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    y = Bidirectional(GRU(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    atten_1 = Attention(MAX_SEQUENCE_LENGTH)(x) # skip connect\n",
    "    atten_2 = Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    conc = Dense(hidDim, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(conc)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-2),\n",
    "                  metrics=[microF1Loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTriModel(embeddingMatrix, hidDim=32, learnEmbs=False):\n",
    "    if learnEmbs:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH)\n",
    "    else:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embeddingMatrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embeddingLayer(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    y = Bidirectional(GRU(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    atten_1 = Attention(MAX_SEQUENCE_LENGTH)(x) # skip connect\n",
    "    atten_2 = Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    conc = Dense(hidDim, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=conc)\n",
    "    \n",
    "    input_l, input_m, input_r = Input((MAX_SEQUENCE_LENGTH,)), Input((MAX_SEQUENCE_LENGTH,)), Input((MAX_SEQUENCE_LENGTH,))\n",
    "    embed_l, embed_m, embed_r = model(input_l), model(input_m), model(input_r)\n",
    "    output = Average()([embed_l, embed_m, embed_r])\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(output)\n",
    "    tri_model = Model(inputs=[input_l, input_m, input_r], outputs=output)\n",
    "    tri_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-2),\n",
    "                  metrics=[microF1Loss])\n",
    "    return tri_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBiModel(embeddingMatrix=None, hidDim=64, batchNorm=False, needf1=True):\n",
    "    if embeddingMatrix is not None:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embeddingMatrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    else:\n",
    "        embeddingLayer = Embedding(100,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embeddingLayer(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    y = Bidirectional(GRU(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    atten_1 = Attention(MAX_SEQUENCE_LENGTH)(x) # skip connect\n",
    "    atten_2 = Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    conc = Dense(hidDim)(conc)\n",
    "    if batchNorm:\n",
    "        conc = BatchNormalization()(conc)\n",
    "    conc = Activation('relu')(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=conc)\n",
    "    \n",
    "    input_l, input_r = Input((MAX_SEQUENCE_LENGTH,)), Input((MAX_SEQUENCE_LENGTH,))\n",
    "    embed_l, embed_r = model(input_l), model(input_r)\n",
    "#     output = Average()([embed_l, embed_r])\n",
    "    output = concatenate([embed_l, embed_r])\n",
    "    output = Dropout(0.25)(output)\n",
    "    if needf1:\n",
    "        output = Dense(NUM_CLASSES, activation='softmax')(output)\n",
    "    else:\n",
    "        output = Dense(1, activation='sigmoid')(output)\n",
    "    bi_model = Model(inputs=[input_l, input_r], outputs=output)\n",
    "    if needf1:\n",
    "        bi_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-2),\n",
    "                  metrics=[microF1Loss])\n",
    "    else:\n",
    "        bi_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    return bi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss (cost function):\n",
      "training   (min:    0.400, max:    0.723, cur:    0.422)\n",
      "validation (min:    0.328, max:    0.538, cur:    0.426)\n",
      "\n",
      "microF1Loss:\n",
      "training   (min:    0.808, max:    0.833, cur:    0.822)\n",
      "validation (min:    0.537, max:    0.636, cur:    0.597)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7f157a208>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from livelossplot.keras import PlotLossesCallback\n",
    "\n",
    "class_weight = {0: len(labels)/np.sum(np.argmax(labels, axis=1)!=1),\n",
    "                1: len(labels)/np.sum(np.argmax(labels, axis=1)==1)}\n",
    "\n",
    "model = buildSingleModel(embeddingMatrix, hidDim=128, learnEmbs=False)\n",
    "model.fit(train_all, labels, batch_size=128, epochs=10,\n",
    "          callbacks=[PlotLossesCallback()], verbose=0,\n",
    "          validation_data=(test_all, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_meta_features(data):\n",
    "    # Compute emoji-based features\n",
    "    emojis = ['😂', '😭', '😞', '😢', '😁', '😅', '😍',\n",
    "              '😀', '😃', '😡', '😄', '😆', '😒', '😊',\n",
    "              '😌', '😠', '😤', '🙂', '😺', '😫', '😩',\n",
    "              '😹', '😜', '👍', '😘', '😸', '😉', '😽',\n",
    "              '😻', '😏', '💔', '😝', '😑', '🙁', '😾',\n",
    "              '😿', '😬', '❤', '😋', '🙄', '😔', '🙀',\n",
    "              '😎', '👎', '😦', '😧', '❤️', '😛', '😶',\n",
    "              '😐', '👌', '🤔','😇', '😨', '😯', '😳',\n",
    "              '☹️', '💋', '👋', '😪', '😥', '💕', '😱',\n",
    "              '🙈', '😟', '🙏', '✌', '😖', '😣', '😮',\n",
    "              '🤗', '😓', '😷', '☹', '💞', '🏻', '🙌',\n",
    "              '💐', '🙊', '😰', '☺', '😴', '🖕', '♥', '😕',\n",
    "              '😈', '💗', '♡', '👀', '👊', '‑c', ' 8‑d', ' ‑d',\n",
    "              '👻', '：）', '.', '?', '!', ',', '-', '・', \"'-'\",\n",
    "              '\\U0001f923','・ω・', '\\U000fe339', ' ‑c']\n",
    "    happy_words = ['happy', 'lol', 'haha', 'enjoy', 'cool', 'glad',\n",
    "                   'smile', 'nice', 'funny', 'wow', 'good', 'best',\n",
    "                   'party', 'baby', 'sweet', 'joke', 'glad', 'perfect',\n",
    "                   'fantastic', 'excite', 'cute', 'enjoy', 'omg']\n",
    "    angry_words = ['angry', 'fuck', 'hell', 'shut up', 'bad', 'rude',\n",
    "                  'block', 'stupid', 'piss', 'lame', \"don't\", 'mean',\n",
    "                  'irritat', 'hate', 'ignore', 'get lost', 'reply',\n",
    "                  'fool', 'regret', 'dumb', 'cheat', 'whore', 'disgust']\n",
    "    sad_words   = ['sad', 'sorry', 'miss', 'alone', 'lonely', 'cry',\n",
    "                   'disappointed', 'not', 'no', 'not happy', 'crazy',\n",
    "                   'stress', 'depress', 'poor', 'care', 'health', 'break up',\n",
    "                   'breaking up', 'upset', 'forgive', 'left me', 'dump']\n",
    "    others_words = ['thank you', 'favorite', 'favourite']\n",
    "    indicator_words = emojis + happy_words + angry_words + sad_words + others_words\n",
    "    \n",
    "    word_features = np.zeros((len(data), len(indicator_words)))\n",
    "    for i, text in enumerate(data):\n",
    "        for j, word in enumerate(indicator_words):\n",
    "            useful_text = text.lower()\n",
    "#             useful_text = \" \".join([text.split(' <eos> ')[0], text.split(' <eos> ')[-1]]).lower()\n",
    "            word_features[i][j] += useful_text.count(word)\n",
    "    \n",
    "    # Compute CAPS-based features\n",
    "    capital_features = np.zeros((len(data), 3))\n",
    "    for i, text in enumerate(data):\n",
    "        for word in text.split(' '):\n",
    "            if word.isupper():\n",
    "                capital_features[i][0] += 1\n",
    "        capital_features[i][1] = capital_features[i][0] / (len(text.split(' ')) + 1)\n",
    "        capital_features[i][2] = sum([len(x) for x in text.split(' ')]) / len(text.split(' '))\n",
    "    \n",
    "    # Combine metadata-based features\n",
    "    metadata_features = np.concatenate((word_features, capital_features), axis=1)\n",
    "    return metadata_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30160, 180)\n"
     ]
    }
   ],
   "source": [
    "metadata_features = construct_meta_features(rawtrainTexts)\n",
    "print(metadata_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [14930.  4063.  5074.  5246.]\n",
      "False Positives per class :  [787.  12.  44.   4.]\n",
      "False Negatives per class :  [ 18. 180. 389. 260.]\n",
      "Class happy : Precision : 0.997, Recall : 0.958, F1 : 0.977\n",
      "Class sad : Precision : 0.991, Recall : 0.929, F1 : 0.959\n",
      "Class angry : Precision : 0.999, Recall : 0.953, F1 : 0.975\n",
      "Ignoring the Others class, Macro Precision : 0.9959, Macro Recall : 0.9464, Macro F1 : 0.9705\n",
      "Ignoring the Others class, Micro TP : 14383, FP : 60, FN : 829\n",
      "Accuracy : 0.9719, Micro Precision : 0.9958, Micro Recall : 0.9455, Micro F1 : 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9719164456233422, 0.99584574, 0.94550353, 0.9700219082279653)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier for metadata-based classification\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "meta_clf = tree.DecisionTreeClassifier()\n",
    "meta_clf.fit(metadata_features, np.argmax(labels, axis=1))\n",
    "getMetrics(meta_clf.predict_proba(metadata_features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train accuracy with and without emoji-augmentation\n",
    "# preds = model.predict([train_l, train_m, train_r], batch_size=1024)\n",
    "preds = model.predict(train_all, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_augment = meta_clf.predict_proba(metadata_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.predict([test_l, test_m, test_r], batch_size=1024)\n",
    "# predictions = model.predict([test_l, test_r], batch_size=1024)\n",
    "predictions = model.predict(test_all, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = combined_clf.predict_proba(combined_test_features)\n",
    "\n",
    "test_meta = construct_meta_features(rawtestTexts)\n",
    "predictions_meta = meta_clf.predict_proba(test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 0. 62.  0.  0.]\n",
      "False Positives per class :  [ 0. 40.  0.  0.]\n",
      "False Negatives per class :  [39.  0.  1.  0.]\n",
      "Class happy : Precision : 0.608, Recall : 1.000, F1 : 0.756\n",
      "Class sad : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class angry : Precision : nan, Recall : nan, F1 : 0.000\n",
      "Ignoring the Others class, Macro Precision : nan, Macro Recall : nan, Macro F1 : 0.0000\n",
      "Ignoring the Others class, Micro TP : 62, FP : 40, FN : 1\n",
      "Accuracy : 0.6078, Micro Precision : 0.6078, Micro Recall : 0.9841, Micro F1 : 0.7515\n",
      "\n",
      "True Positives per class :  [ 0.  0. 72.  0.]\n",
      "False Positives per class :  [ 0.  0. 30.  0.]\n",
      "False Negatives per class :  [28.  1.  0.  1.]\n",
      "Class happy : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class sad : Precision : 0.706, Recall : 1.000, F1 : 0.828\n",
      "Class angry : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Ignoring the Others class, Macro Precision : nan, Macro Recall : 0.3333, Macro F1 : 0.0000\n",
      "Ignoring the Others class, Micro TP : 72, FP : 30, FN : 2\n",
      "Accuracy : 0.7059, Micro Precision : 0.7059, Micro Recall : 0.9730, Micro F1 : 0.8182\n",
      "\n",
      "True Positives per class :  [ 0.  0.  0. 95.]\n",
      "False Positives per class :  [ 0.  0.  0. 31.]\n",
      "False Negatives per class :  [28.  1.  2.  0.]\n",
      "Class happy : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class sad : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class angry : Precision : 0.754, Recall : 1.000, F1 : 0.860\n",
      "Ignoring the Others class, Macro Precision : nan, Macro Recall : 0.3333, Macro F1 : 0.0000\n",
      "Ignoring the Others class, Micro TP : 95, FP : 31, FN : 3\n",
      "Accuracy : 0.7540, Micro Precision : 0.7540, Micro Recall : 0.9694, Micro F1 : 0.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paragag/persona/lib/python3.5/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in float_scalars\n",
      "/home/paragag/persona/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.753968253968254, 0.75396824, 0.96938777, 0.848214281722903)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See F1 scores for varying threshold values of max pred value\n",
    "# print(sorted(predictions[:,1], reverse=True))\n",
    "consider_indices = np.where(predictions[:,1] >= 0.8)[0]\n",
    "getMetrics(predictions[consider_indices], testLabels[consider_indices])\n",
    "print()\n",
    "consider_indices = np.where(predictions[:,2] >= 0.6)[0]\n",
    "getMetrics(predictions[consider_indices], testLabels[consider_indices])\n",
    "print()\n",
    "consider_indices = np.where(predictions[:,3] >= 0.9)[0]\n",
    "getMetrics(predictions[consider_indices], testLabels[consider_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [1719.  101.   96.  118.]\n",
      "False Positives per class :  [ 81. 208. 241. 191.]\n",
      "False Negatives per class :  [619.  41.  29.  32.]\n",
      "Class happy : Precision : 0.327, Recall : 0.711, F1 : 0.448\n",
      "Class sad : Precision : 0.285, Recall : 0.768, F1 : 0.416\n",
      "Class angry : Precision : 0.382, Recall : 0.787, F1 : 0.514\n",
      "Ignoring the Others class, Macro Precision : 0.3312, Macro Recall : 0.7553, Macro F1 : 0.4605\n",
      "Ignoring the Others class, Micro TP : 315, FP : 640, FN : 102\n",
      "Accuracy : 0.7383, Micro Precision : 0.3298, Micro Recall : 0.7554, Micro F1 : 0.4592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7382940108892923, 0.32984293, 0.7553957, 0.4591836465880487)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_handcrafted = []\n",
    "for i, pred in enumerate(predictions):\n",
    "    if pred[1] >= 0.8:\n",
    "        predictions_handcrafted.append([0, 1, 0, 0])\n",
    "    elif pred[2] >= 0.6:\n",
    "        predictions_handcrafted.append([0, 0, 1, 0])\n",
    "    elif pred[3] >= 0.9:\n",
    "        predictions_handcrafted.append([0, 0, 0, 1])\n",
    "    else:\n",
    "#         predictions_handcrafted.append([1, 0, 0, 0])\n",
    "#         predictions_handcrafted.append(pred)\n",
    "#         print(np.argmax(predictions_meta[i]), np.argmax(pred))\n",
    "        predictions_handcrafted.append(predictions_meta[i])\n",
    "\n",
    "predictions_handcrafted = np.array(predictions_handcrafted)\n",
    "getMetrics(predictions_handcrafted, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [2029.   95.   98.  130.]\n",
      "False Positives per class :  [ 67. 138. 103.  95.]\n",
      "False Negatives per class :  [309.  47.  27.  20.]\n",
      "Class happy : Precision : 0.408, Recall : 0.669, F1 : 0.507\n",
      "Class sad : Precision : 0.488, Recall : 0.784, F1 : 0.601\n",
      "Class angry : Precision : 0.578, Recall : 0.867, F1 : 0.693\n",
      "Ignoring the Others class, Macro Precision : 0.4910, Macro Recall : 0.7732, Macro F1 : 0.6006\n",
      "Ignoring the Others class, Micro TP : 323, FP : 336, FN : 94\n",
      "Accuracy : 0.8537, Micro Precision : 0.4901, Micro Recall : 0.7746, Micro F1 : 0.6004\n",
      "\n",
      "True Positives per class :  [2217.   72.   94.  113.]\n",
      "False Positives per class :  [124.  41.  49.  45.]\n",
      "False Negatives per class :  [121.  70.  31.  37.]\n",
      "Class happy : Precision : 0.637, Recall : 0.507, F1 : 0.565\n",
      "Class sad : Precision : 0.657, Recall : 0.752, F1 : 0.701\n",
      "Class angry : Precision : 0.715, Recall : 0.753, F1 : 0.734\n",
      "Ignoring the Others class, Macro Precision : 0.6699, Macro Recall : 0.6708, Macro F1 : 0.6703\n",
      "Ignoring the Others class, Micro TP : 279, FP : 135, FN : 138\n",
      "Accuracy : 0.9060, Micro Precision : 0.6739, Micro Recall : 0.6691, Micro F1 : 0.6715\n",
      "\n",
      "True Positives per class :  [2219.   71.   95.  113.]\n",
      "False Positives per class :  [124.  40.  49.  44.]\n",
      "False Negatives per class :  [119.  71.  30.  37.]\n",
      "Class happy : Precision : 0.640, Recall : 0.500, F1 : 0.561\n",
      "Class sad : Precision : 0.660, Recall : 0.760, F1 : 0.706\n",
      "Class angry : Precision : 0.720, Recall : 0.753, F1 : 0.736\n",
      "Ignoring the Others class, Macro Precision : 0.6730, Macro Recall : 0.6711, Macro F1 : 0.6721\n",
      "Ignoring the Others class, Micro TP : 279, FP : 133, FN : 138\n",
      "Accuracy : 0.9067, Micro Precision : 0.6772, Micro Recall : 0.6691, Micro F1 : 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9067150635208712, 0.67718446, 0.66906476, 0.6731001249880632)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMetrics(predictions, testLabels)\n",
    "print()\n",
    "predictions_sly = np.copy(predictions)\n",
    "for i in range(len(predictions_sly)):\n",
    "    if predictions_sly[i][0] >= 0.2:\n",
    "        predictions_sly[i] = [1, 0, 0, 0]\n",
    "    elif predictions_sly[i][2] >= 0.3:\n",
    "        predictions_sly[i] = [0, 0, 1, 0]\n",
    "alpha = 0.7\n",
    "getMetrics(predictions_sly, testLabels)\n",
    "print()\n",
    "getMetrics(alpha * predictions_sly + (1-alpha) * predictions_meta, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions  = 0.5 * predictions + 0.5 * predictions_meta\n",
    "with io.open(\"./parag_preds.txt\", \"w\", encoding=\"utf8\") as fout:\n",
    "    for pred in predictions:\n",
    "        fout.write('\\t'.join([str(x) for x in pred]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "predictions_towrite = (predictions).argmax(axis=1)\n",
    "# predictions_towrite = (alpha * predictions_sly + (1-alpha) * predictions_meta).argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[predictions_towrite[lineNum]] + '\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = [0, 0, 0]\n",
    "set_len = [15, 15, 15]\n",
    "for x in trainTexts:\n",
    "    parts = x.split(' <eos> ')\n",
    "    for j in range(3):\n",
    "        if len(parts[j].split(' ')) <= set_len[j]:\n",
    "            avg_len[j] += 1\n",
    "print([avg_len[i]/len(trainTexts) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona",
   "language": "python",
   "name": "persona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
