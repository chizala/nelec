{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import concatenate, Activation, GlobalAveragePooling1D, GlobalMaxPooling1D, Layer, Dense, Embedding, LSTM, GRU, Dropout, SpatialDropout1D, Input, Average, Bidirectional, BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import utils\n",
    "\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't hog GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training and testing data file. This data can be downloaded from a link, details of which will be provided.\n",
    "trainDataPath = \"./train.txt\"\n",
    "testDataPath = \"./dev.txt\"\n",
    "evalDataPath = \"./evaluate.txt\"\n",
    "# Output file that will be generated. This file can be directly submitted.\n",
    "solutionPath = \"./test.txt\"\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4                 # Number of classes - Happy, Sad, Angry, Others\n",
    "MAX_NB_WORDS = 15000                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer \n",
    "MAX_SEQUENCE_LENGTH = 35         # All sentences having lesser number of words than this will be padded\n",
    "EMBEDDING_DIM = 300               # The dimension of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        raw_conversations : All conversations together\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    raw_conversations = []\n",
    "    labels = []\n",
    "    \n",
    "    importlib.reload(sys.modules['regex'])\n",
    "    import regex\n",
    "    \n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            raw_conv = ' '.join(line[:].strip().split('\\t')[1:4])\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            # Remove stray punctuation\n",
    "            stray_punct = ['‑', '-', \"^\", \":\",\n",
    "                           \";\", \"#\", \")\", \"(\", \"*\", \"=\", \"\\\\\", \"/\"]\n",
    "            for punct in stray_punct:\n",
    "                    conv = conv.replace(punct, \"\")\n",
    "    \n",
    "            processedData = regex.cleanText(conv.lower(), remEmojis=False).lower() #.rstrip()\n",
    "            processedData = processedData.replace(\"'\", \"\")\n",
    "            # Remove numbers\n",
    "            processedData = ''.join([i for i in processedData if not i.isdigit()])\n",
    "\n",
    "            conversations.append(processedData)\n",
    "            raw_conversations.append(raw_conv)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, raw_conversations, labels\n",
    "    else:\n",
    "        return indices, conversations, raw_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Loading utils module\n",
      "Processing test data...\n",
      "Processing evaluation data...\n",
      "Extracting tokens...\n",
      "Found 18018 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, trainTexts, rawtrainTexts, labels = preprocessData(trainDataPath, mode=\"train\")\n",
    "labels = to_categorical(np.asarray(labels), NUM_CLASSES)\n",
    "print(\"Processing test data...\")\n",
    "testIndices, testTexts, rawtestTexts, testLabels = preprocessData(testDataPath, mode=\"train\")\n",
    "testLabels = to_categorical(np.asarray(testLabels), NUM_CLASSES)\n",
    "print(\"Processing evaluation data...\")\n",
    "evalIndices, evalTexts, rawevalTexts = preprocessData(evalDataPath, mode=\"test\")\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(trainTexts)\n",
    "tokenizer.fit_on_texts(trainTexts + testTexts + evalTexts)\n",
    "trainSequences = tokenizer.texts_to_sequences(trainTexts)\n",
    "testSequences = tokenizer.texts_to_sequences(testTexts)\n",
    "evalSequences = tokenizer.texts_to_sequences(evalTexts)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "with io.open(\"./parag_train.txt\", \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"turn1\", \"turn2\", \"turn3\"]) + '\\n')        \n",
    "    for i in range(len(trainTexts)):\n",
    "        turn1, turn2, turn3 = trainTexts[i].split(' <eos> ')\n",
    "        fout.write('\\t'.join([turn1, turn2, turn3]))\n",
    "        fout.write('\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "with io.open(\"./parag_val.txt\", \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"turn1\", \"turn2\", \"turn3\"]) + '\\n')        \n",
    "    for i in range(len(testTexts)):\n",
    "        turn1, turn2, turn3 = testTexts[i].split(' <eos> ')\n",
    "        fout.write('\\t'.join([turn1, turn2, turn3]))\n",
    "        fout.write('\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "with io.open(\"./parag_eval.txt\", \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"turn1\", \"turn2\", \"turn3\"]) + '\\n')        \n",
    "    for i in range(len(evalTexts)):\n",
    "        turn1, turn2, turn3 = evalTexts[i].split(' <eos> ')\n",
    "        fout.write('\\t'.join([turn1, turn2, turn3]))\n",
    "        fout.write('\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-data Coverage (cutoff length): 0.9904509283819629\n",
      "Test-data Coverage (cutoff length): 0.992377495462795\n",
      "Eval-data Coverage (cutoff length): 0.9920130695225994\n"
     ]
    }
   ],
   "source": [
    "lens = [len(x) for x in trainSequences]\n",
    "print(\"Train-data Coverage (cutoff length):\", np.sum(np.array(lens) <= MAX_SEQUENCE_LENGTH) / len(trainSequences))\n",
    "lens = [len(x) for x in testSequences]\n",
    "print(\"Test-data Coverage (cutoff length):\", np.sum(np.array(lens) <= MAX_SEQUENCE_LENGTH) / len(testSequences))\n",
    "lens = [len(x) for x in evalSequences]\n",
    "print(\"Eval-data Coverage (cutoff length):\", np.sum(np.array(lens) <= MAX_SEQUENCE_LENGTH) / len(evalSequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage with 15000 words: 0.9987336272485694\n"
     ]
    }
   ],
   "source": [
    "sorted_wordcounts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "words_covered, total_words = 0, 0\n",
    "for i, tup in enumerate(sorted_wordcounts):\n",
    "    total_words += tup[1]\n",
    "    if i < MAX_NB_WORDS:\n",
    "        words_covered += tup[1]\n",
    "print(\"Coverage with %d words:\" % MAX_NB_WORDS, words_covered/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_wordcounts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "with open('./need_sswe.txt', 'w') as f:\n",
    "    for i, tup in enumerate(sorted_wordcounts):\n",
    "        f.write(tup[0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l, train_m, train_r = utils.split_into_three(trainTexts, tokenizer)\n",
    "test_l, test_m, test_r = utils.split_into_three(testTexts, tokenizer)\n",
    "eval_l, eval_m, eval_r = utils.split_into_three(evalTexts, tokenizer)\n",
    "\n",
    "train_all = tokenizer.texts_to_sequences(trainTexts)\n",
    "test_all = tokenizer.texts_to_sequences(testTexts)\n",
    "eval_all = tokenizer.texts_to_sequences(evalTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wont you ask my age  <eos> hey at least i age well  <eos> can you tell me how can we get closer  \n",
      "Wont u ask my age?? hey at least I age well! Can u tell me how can we get closer??\n",
      "\n",
      "i said yes <eos> what if i told you iam not  <eos> go to hell\n",
      "I said yes What if I told you I'm not? Go to hell\n",
      "\n",
      "where i ll check <eos> why tomorrow  <eos> no i want now\n",
      "Where I ll check why tomorrow? No I want now\n",
      "\n",
      "shall we meet <eos> you say you are leaving soon  anywhere you wanna go before you head  <eos>  \n",
      "Shall we meet you say- you're leaving soon...anywhere you wanna go before you head? ?\n",
      "\n",
      "let us change the subject <eos> i just did it  l  <eos> you are broken\n",
      "Let's change the subject I just did it .l. You're broken\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    print(testTexts[i])\n",
    "    print(rawtestTexts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Found 400000 word vectors.\n",
      "Found embedding for 76.16175723717623 % embeddings\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix, oov = utils.getEmbeddingMatrix(wordIndex, EMBEDDING_DIM)\n",
    "oov = [(x, tokenizer.word_counts.get(x, 0)) for x in oov]\n",
    "oov.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils module\n",
      "Populating SSWE embedding matrix...\n",
      "Found unreadable 144 word vectors\n",
      "Found 15936 word vectors.\n",
      "Found embedding for 98.99060436769933 % embeddings\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sys.modules['utils'])\n",
    "import utils\n",
    "\n",
    "print(\"Populating SSWE embedding matrix...\")\n",
    "embeddingMatrix_sswe, oov_sswe = utils.get_sswe_embeddings(wordIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['≠', 'afterdays', '…it', 'more…lasagna', 'family…', 'görlitz', 'rşch', 'ilm', 'whére', 'ω', 'girlfriend\\u200b', 'iş', 'wai', '⊄', 'referrel', 'iscoming', 'me…', '¡so', '¿did', 'aadu', 'face…', 'n', 'xx', 'nan', 'libspill', 'serious…just', 'mbile', 'bred', 'ounces', 'r', '••', 'yyi', 'song¿', 'elhamdülillah', '\\u200d\\u200d', 'okay\\u200b', 'what¡', 'well…', 'one—', 'i̇', 'àapka', 'kha\\u200d\\u200d', 'ı', 'friendsqa', '–', 'vho', 'not\\u200b', 'kķkkkk', 'vmyhorbs', 'driver', 'it¿', 'ĺove', '—', 'cross\\u200b', 'still…', '¶', 'party¿', 'sad…', 'will…', 'cbillion', '͡°', 'alented', 'snt', 'itӳ', 'false', 'któw', 'morrow', 'i…', '͜ʖ', '×', 'wuakd', '฿', 'yaaaaà', 'what¿¿', 'ixm', 'is…', '\\u200d\\u200d\\u200d\\u200d', 'netflixs\\u200b', 'doğng', 'mnts', 'seelfi', 'sone', 'cliché', 'same…', 'or…', 'beşiktaş', 'vsit', 'gudni', 'yr', 'want…', 'north\\u200b', 'rknow', '¿', 'su', 'kiu', 'y', '∑oo', 'çuz', 'fi̇nd', 'gr']\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "print(oov_sswe[:100])\n",
    "print(len(oov_sswe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emoji', 39), ('i̇', 18), ('emojis', 12), ('friendzoned', 11), ('friendzone', 10), ('tajmahal', 9), ('hmmzoning', 9), ('oho', 9), ('donapost', 8), ('lolz', 8), ('rted', 7), ('bangaram', 7), ('everyones', 7), ('ehh', 7), ('gng', 7), ('everythings', 6), ('useropenreflink', 6), ('nonveg', 6), ('whatsaap', 6), ('iand', 5), ('thatll', 5), ('ftfy', 5), ('dafaq', 5), ('facepalm', 5), ('whatt', 5), ('oww', 5), ('ddlj', 5), ('selfies', 5), ('freecharge', 5), ('chatbots', 5), ('\\u200d', 5), ('emojisong', 4), ('himher', 4), ('begar', 4), ('happies', 4), ('thnk', 4), ('wiil', 4), ('lololol', 4), ('playcreepypedia', 4), ('flipkart', 4), ('temme', 4), ('bdw', 4), ('youve', 4), ('brokeup', 4), ('oclock', 4), ('lolzzz', 4), ('whyre', 4), ('habbit', 4), ('iaposm', 4), ('whts', 4), ('xams', 4), ('medam', 4), ('arjit', 4), ('bhindi', 3), ('lve', 3), ('dobt', 3), ('theyll', 3), ('padmavat', 3), ('hihihi', 3), ('scuffletown', 3), ('katachi', 3), ('tysm', 3), ('baaghi', 3), ('okz', 3), ('brozoned', 3), ('hinglish', 3), ('fucken', 3), ('skys', 3), ('watsaap', 3), ('goid', 3), ('ranbeer', 3), ('tshirts', 3), ('hyd', 3), ('typelaugh', 3), ('maam', 3), ('wheather', 3), ('ohhhooo', 3), ('nothng', 3), ('thatd', 3), ('tonights', 3), ('halfgirlfriend', 3), ('werent', 3), ('huhhhh', 3), ('ssup', 3), ('wch', 3), ('anyones', 3), ('okiee', 3), ('howd', 3), ('besties', 3), ('sned', 3), ('offf', 3), ('youself', 3), ('itll', 3), ('selfi', 3), ('frind', 3), ('uve', 3), ('awwh', 3), ('lols', 3), ('nightcore', 3), ('shits', 3)]\n",
      "3755\n"
     ]
    }
   ],
   "source": [
    "print(oov[:100])\n",
    "print(len(oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooh = {}\n",
    "\n",
    "relevant_oov = [x[0] for x in oov]\n",
    "actual_words = [k for (k,v) in tokenizer.word_index.items()]\n",
    "for datum in testTexts:\n",
    "    words = datum.split(' ')\n",
    "    for word in words:\n",
    "        if word in relevant_oov :\n",
    "            ooh[word] = ooh.get(word, 0) + 1\n",
    "\n",
    "ooh_items = [(k, v) for (k,v) in ooh.items()]\n",
    "ooh_items.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('donapost', 6), ('iaposm', 3), ('friendzoned', 3), ('emojis', 3), ('nightcore', 3), ('frnz', 2), ('wowe', 2), ('whataposs', 2), ('ahow', 2), ('bachhe', 2), ('tajmahal', 2), ('playcreepypedia', 2), ('emojisong', 2), ('oclock', 2), ('lolz', 2), ('emoji', 2), ('kewlwst', 2), ('baaghi', 2), ('lamakaan', 2), ('swiggy', 1), ('dunp', 1), ('howaposs', 1), ('hmmzoning', 1), ('oopsy', 1), ('wokayyy', 1), ('vakola', 1), ('lolzzz', 1), ('tindering', 1), ('dhyat', 1), ('besties', 1), ('ocourse', 1), ('whoaposs', 1), ('tnq', 1), ('sharukh', 1), ('yaantey', 1), ('juxt', 1), ('thks', 1), ('milovie', 1), ('xwhat', 1), ('muhhhh', 1), ('higherres', 1), ('megalol', 1), ('areee', 1), ('pshhhh', 1), ('aahh', 1), ('powerliftingsmh', 1), ('tranwendy', 1), ('ivve', 1), ('one—', 1), ('temme', 1), ('saxy', 1), ('fucken', 1), ('googlin', 1), ('dped', 1), ('aaare', 1), ('introducted', 1), ('sircasm', 1), ('favoriet', 1), ('scense', 1), ('gafe', 1), ('gulthfriend', 1), ('whatsaap', 1), ('fidha', 1), ('berozgaari', 1), ('pizzaaaass', 1), ('onely', 1), ('sabji', 1), ('pulka', 1), ('littl', 1), ('frenching', 1), ('kisse', 1), ('caming', 1), ('pokiri', 1), ('interperted', 1), ('soported', 1), ('whate', 1), ('yediiiii', 1), ('unaru', 1), ('nect', 1), ('sharaddha', 1), ('lovery', 1), ('strtd', 1), ('bthing', 1), ('ahve', 1), ('sagarwcam', 1), ('vandetta', 1), ('aiyyo', 1), ('wtsup', 1), ('deepikas', 1), ('friendzoning', 1), ('agian', 1), ('dubstub', 1), ('everyones', 1), ('mondegar', 1), ('cortanan', 1), ('tattos', 1), ('zonbie', 1), ('friendzone', 1), ('everyword', 1), ('ledis', 1), ('hahahahaja', 1), ('botking', 1), ('spritiual', 1), ('cuntry', 1), ('dcac', 1), ('happene', 1), ('mny', 1), ('iaposll', 1), ('ugghh', 1), ('krbo', 1), ('rolins', 1), ('charg', 1), ('voicemsg', 1), ('tihri', 1), ('pohot', 1), ('srimathi', 1), ('cryp', 1), ('karunga', 1), ('⃣⃣⃣', 1), ('maam', 1), ('ahaan', 1), ('boujee', 1), ('thataposs', 1), ('plox', 1), ('whatt', 1), ('babyd', 1), ('seld', 1), ('ssup', 1), ('sposed', 1), ('listion', 1), ('ehy', 1), ('siliconbased', 1), ('ruuch', 1), ('youself', 1), ('tumhe', 1), ('≠', 1), ('sovashvssjxj', 1), ('eakada', 1), ('achcha', 1), ('youve', 1), ('ohkaayyy', 1), ('ghoonghat', 1), ('shld', 1), ('spenkig', 1), ('hurman', 1), ('hadnt', 1), ('agyooo', 1), ('cutw', 1), ('yuo', 1), ('bhosdi', 1), ('contect', 1), ('flep', 1), ('ingish', 1), ('whattay', 1), ('zarvise', 1), ('alri', 1), ('afgalgunj', 1), ('referrel', 1), ('skys', 1), ('bdw', 1), ('baggi', 1), ('smouch', 1), ('tatto', 1), ('gve', 1), ('vaoov', 1), ('eminems', 1), ('nabr', 1), ('dout', 1), ('vegita', 1), ('aayojan', 1), ('yyi', 1), ('ishaqbaaz', 1), ('publicuty', 1), ('brozone', 1), ('sunnyleone', 1), ('atrect', 1), ('dued', 1), ('gustion', 1), ('i̇', 1), ('programmin', 1), ('daynight', 1), ('behenchood', 1), ('becauce', 1), ('duckface', 1), ('fuckbuddy', 1), ('lololol', 1), ('hounomkee', 1), ('almigthy', 1), ('sillly', 1), ('opss', 1), ('lovevu', 1), ('answerer', 1), ('whhat', 1), ('whatevr', 1), ('wduwtta', 1), ('edsheerans', 1), ('ironmaiden', 1), ('panipiri', 1), ('mhy', 1), ('nsmer', 1), ('woohooo', 1), ('dafaq', 1), ('rockzz', 1), ('joim', 1), ('ookh', 1), ('lovear', 1), ('kudaellu', 1), ('vettai', 1), ('wuv', 1), ('pleae', 1), ('childo', 1), ('flipkart', 1), ('bakwaas', 1), ('swewty', 1), ('thatd', 1), ('mrunal', 1), ('egyup', 1), ('thatll', 1), ('plutos', 1), ('iscoming', 1), ('wnaa', 1), ('uve', 1), ('underdiving', 1), ('xams', 1), ('adsa', 1), ('partywonderful', 1), ('bhaskars', 1), ('cross\\u200b', 1), ('pming', 1), ('scuffletown', 1), ('bhakkk', 1), ('gujurati', 1), ('tysm', 1), ('àapka', 1), ('fuking', 1), ('mams', 1), ('begar', 1)]\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "print(ooh_items)\n",
    "print(len(ooh_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooh = {}\n",
    "\n",
    "relevant_oov = [x[0] for x in oov]\n",
    "actual_words = [k for (k,v) in tokenizer.word_index.items()]\n",
    "for datum in evalTexts:\n",
    "    words = datum.split(' ')\n",
    "    for word in words:\n",
    "        if word in relevant_oov:\n",
    "            ooh[word] = ooh.get(word, 0) + 1\n",
    "\n",
    "ooh_items = [(k, v) for (k,v) in ooh.items()]\n",
    "ooh_items.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emoji', 5), ('useropenreflink', 3), ('allout', 2), ('everythings', 2), ('friendzoned', 2), ('donapost', 2), ('okiee', 2), ('briyani', 2), ('chatbots', 2), ('halfgirlfriend', 2), ('besties', 2), ('shole', 2), ('iand', 2), ('lolz', 2), ('ftfy', 2), ('panipuri', 2), ('lohiya', 2), ('wachapp', 2), ('choclati', 2), ('i̇', 2), ('hihihi', 2), ('sachhi', 2), ('nigt', 1), ('duuuuude', 1), ('sherikkum', 1), ('whatsa', 1), ('oclock', 1), ('cutti', 1), ('chillll', 1), ('sherlyns', 1), ('friendzone', 1), ('bzy', 1), ('bainchod', 1), ('tgnxmh', 1), ('twilightt', 1), ('shype', 1), ('ihddit', 1), ('shold', 1), ('mattee', 1), ('bwaha', 1), ('kilpauk', 1), ('langwege', 1), ('dhonis', 1), ('ruwho', 1), ('matvh', 1), ('delect', 1), ('directot', 1), ('voicesg', 1), ('blowjob', 1), ('thnk', 1), ('thenn', 1), ('spiling', 1), ('trappp', 1), ('messagee', 1), ('besharam', 1), ('namber', 1), ('llallu', 1), ('aswu', 1), ('toask', 1), ('conformd', 1), ('adhuri', 1), ('heartfull', 1), ('hutiya', 1), ('huhhhh', 1), ('quotfemalequot', 1), ('messanger', 1), ('joffery', 1), ('knowp', 1), ('perioda', 1), ('uask', 1), ('defghi', 1), ('bhenkelode', 1), ('yrah', 1), ('titanfall', 1), ('uhhhmm', 1), ('biddable', 1), ('blockchain', 1), ('iaposm', 1), ('hahaing', 1), ('niceyekyahai', 1), ('sharadha', 1), ('fuuuuuuukkkhhhhh', 1), ('bluddy', 1), ('jantarmantar', 1), ('noshad', 1), ('fuddi', 1), ('arobot', 1), ('kixs', 1), ('wifes', 1), ('jigarthanda', 1), ('okhy', 1), ('aupport', 1), ('virtualasstftw', 1), ('countery', 1), ('fuvvked', 1), ('maledive', 1), ('pffft', 1), ('aritifical', 1), ('simble', 1), ('poguthey', 1)]\n",
      "617\n"
     ]
    }
   ],
   "source": [
    "print(ooh_items[:100])\n",
    "print(len(ooh_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data tensor:  (30160, 35)\n",
      "Shape of training label tensor:  (30160, 4)\n"
     ]
    }
   ],
   "source": [
    "train_l = pad_sequences(train_l, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "train_m = pad_sequences(train_m, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "train_r = pad_sequences(train_r, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "train_all = pad_sequences(train_all, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "test_l = pad_sequences(test_l, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "test_m = pad_sequences(test_m, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "test_r = pad_sequences(test_r, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "test_all = pad_sequences(test_all, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "eval_l = pad_sequences(eval_l, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "eval_m = pad_sequences(eval_m, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "eval_r = pad_sequences(eval_r, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "eval_all = pad_sequences(eval_all, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(\"Shape of training data tensor: \", train_l.shape)\n",
    "print(\"Shape of training label tensor: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils module\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import average\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers import RepeatVector\n",
    "import keras.backend as K\n",
    "\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import utils\n",
    "\n",
    "def buildSingleModel(embeddingMatrix, hidDim=128, dropout=0.2, first_type='lstm', second_type='gru', multitask=False):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    embeddingMatrix.shape[1],\n",
    "                                    weights=[embeddingMatrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embeddingLayer(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    if first_type == 'lstm':\n",
    "        x = Bidirectional(LSTM(hidDim, return_sequences=True,\n",
    "                           dropout=0.2, recurrent_dropout=0.2,\n",
    "                           bias_regularizer=l1_l2(0.01, 0.01),\n",
    "                           recurrent_regularizer=l1_l2(0.01, 0.01),\n",
    "                          ))(x)\n",
    "    else:\n",
    "        x = Bidirectional(GRU(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    if second_type == 'gru':\n",
    "        y = Bidirectional(GRU(hidDim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    else:\n",
    "        y = Bidirectional(LSTM(hidDim, return_sequences=True,\n",
    "                           dropout=0.2, recurrent_dropout=0.2,\n",
    "                           bias_regularizer=l1_l2(0.01, 0.01),\n",
    "                           recurrent_regularizer=l1_l2(0.01, 0.01),\n",
    "                          ))(x)\n",
    "\n",
    "    atten_1 = utils.Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "    atten_2 = utils.Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    avg_pool_1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool_1 = GlobalMaxPooling1D()(x)\n",
    "    avg_pool_2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_2 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    feature_comb_1 = concatenate([atten_1, avg_pool_1, max_pool_1])\n",
    "    feature_comb_1 = Dropout(dropout)(feature_comb_1)\n",
    "    conc_1 = Dense(hidDim, activation=\"relu\")(feature_comb_1)\n",
    "\n",
    "    feature_comb_2 = concatenate([atten_2, avg_pool_2, max_pool_2])\n",
    "    feature_comb_2 = Dropout(dropout)(feature_comb_2)\n",
    "    conc_2 = Dense(hidDim, activation=\"relu\")(feature_comb_2)\n",
    "\n",
    "    conc = concatenate([conc_1, conc_2])\n",
    "    conc = Dropout(dropout)(conc)\n",
    "    conc = Dense(hidDim, activation=\"relu\")(conc)\n",
    "    conc = Dropout(dropout)(conc)\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(conc)\n",
    "    \n",
    "    if multitask:\n",
    "        text_encoding = RepeatVector(MAX_SEQUENCE_LENGTH)(conc)\n",
    "        decoded = LSTM(hidDim, return_sequences=True)(text_encoding)\n",
    "        decoded = LSTM(hidDim, return_sequences=True)(decoded)\n",
    "        decoded_words = Dense(embeddingMatrix.shape[0], activation='softmax')(decoded)\n",
    "        \n",
    "        def multitask_loss(y_true, y_pred):\n",
    "            cross_entropy_loss = K.categorical_crossentropy(y_true[1], y_pred[1])\n",
    "            autoencoder_loss = K.mean(K.square(y_true[0] - y_pred[0]))\n",
    "            combined_loss = cross_entropy_loss + autoencoder_loss\n",
    "            return combined_loss\n",
    "        \n",
    "        auto_model = Model(inputs=inp, outputs=decoded_words)\n",
    "        main_model = Model(inputs=inp, outputs=output)\n",
    "        auto_model.compile(loss='mse',\n",
    "                  optimizer=optimizers.Adam(1e-3),\n",
    "                  metrics=['accuracy'])\n",
    "        main_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-3),\n",
    "                  metrics=[utils.microF1Loss])\n",
    "        return auto_model, main_model\n",
    "    else:\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-3),\n",
    "                  metrics=[utils.microF1Loss])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import average\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "def buildSingleDEEPModel(embeddingMatrix, hidDim=128, learnEmbs=False, dropout=0.2):\n",
    "    if learnEmbs:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    embeddingMatrix.shape[1],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH)\n",
    "    else:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    embeddingMatrix.shape[1],\n",
    "                                    weights=[embeddingMatrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embeddingLayer(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "#     x = Bidirectional(LSTM(hidDim, return_sequences=True,\n",
    "#                            dropout=0.1, recurrent_dropout=0.1,\n",
    "#                            bias_regularizer=l1(0.01),\n",
    "#                            recurrent_regularizer=l2(0.01),\n",
    "#                           ))(x)\n",
    "#     x = Bidirectional(LSTM(hidDim, return_sequences=True,\n",
    "#                            dropout=0.1, recurrent_dropout=0.1,\n",
    "#                            bias_regularizer=l1(0.01),\n",
    "#                            recurrent_regularizer=l2(0.01),\n",
    "#                           ))(x)\n",
    "    \n",
    "    x = LSTM(hidDim, return_sequences=False,\n",
    "                           dropout=0.1, recurrent_dropout=0.1,\n",
    "                           bias_regularizer=l1_l2(0.01, 0.01),\n",
    "                           recurrent_regularizer=l1_l2(0.01, 0.01),\n",
    "                          )(x)\n",
    "#     x = Bidirectional(LSTM(hidDim, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "#     x = Bidirectional(LSTM(hidDim, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))(x)    \n",
    "#     x = utils.Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "    \n",
    "    conc = Dropout(dropout)(x)\n",
    "    conc = Dense(hidDim, activation=\"relu\")(conc)\n",
    "    conc = Dropout(dropout)(conc)\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(conc)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-3),\n",
    "#                   optimizer=optimizers.Adam(1e-4),\n",
    "#                   optimizer=optimizers.SGD(5e-3),\n",
    "                  metrics=[utils.microF1Loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.09015739790282193, 1: 0.35540724746822805, 2: 0.27889419830855045, 3: 0.27554115632039955}\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 0.25,\n",
    "                1: 0.25,\n",
    "                2: 0.25,\n",
    "                3: 0.25}\n",
    "total = 0\n",
    "for i in range(4):\n",
    "    class_weight[i] = (len(labels) + len(testLabels))/(np.sum(np.argmax(labels, 1)==i)+np.sum(np.argmax(testLabels, 1)==i))\n",
    "    total += class_weight[i]\n",
    "\n",
    "for i in range(4):\n",
    "    class_weight[i] /= total\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30160\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-673-259aa9d639cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPlotLossesCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           validation_data=(test_all, testLabels))\n\u001b[0m",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    217\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/livelossplot/generic_keras.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliveplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliveplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/livelossplot/generic_plot.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   \u001b[0mmetric2title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric2title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                   \u001b[0mextrema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_extrema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                   fig_path=self.fig_path)\n\u001b[0m",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/livelossplot/core.py\u001b[0m in \u001b[0;36mdraw_plot\u001b[0;34m(logs, metrics, figsize, max_epoch, max_cols, validation_fmt, metric2title, extrema, fig_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfig_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextrema_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2224\u001b[0m                         \u001b[0mclip_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mclip_box\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2226\u001b[0;31m                             \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2227\u001b[0m                         \u001b[0mclip_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mclip_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mintersection\u001b[0;34m(bbox1, bbox2)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mintersect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \"\"\"\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mxmin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mleft\u001b[0m \u001b[0medge\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbounding\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \"\"\"\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mget_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                  \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                  \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m                  [p[1, 0], p[1, 1]]])\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from livelossplot.keras import PlotLossesCallback\n",
    "\n",
    "clr = utils.CyclicLR(base_lr=0.001, max_lr=0.005,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "model = buildSingleModel(embeddingMatrix_sswe, hidDim=32,\n",
    "                         dropout=0.25, multitask=False,\n",
    "                        first_type='lstm', second_type='gru')\n",
    "model.fit(train_all, labels,\n",
    "          batch_size=2048, epochs=150,\n",
    "          class_weight=class_weight,\n",
    "          callbacks=[PlotLossesCallback(), clr],\n",
    "          verbose=0,\n",
    "          validation_data=(test_all, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "\n",
    "def get_my_model():\n",
    "    print(\"Building model\")\n",
    "    return buildSingleModel(embeddingMatrix_sswe, hidDim=32,\n",
    "                         dropout=0.25, multitask=False,\n",
    "                        first_type='lstm', second_type='gru')\n",
    "\n",
    "classifier = KerasClassifier(get_my_model)\n",
    "\n",
    "n_initial = 10000\n",
    "initial_idx = np.random.choice(range(len(train_all)), size=n_initial, replace=False)\n",
    "X_initial = train_all[initial_idx]\n",
    "y_initial = labels[initial_idx]\n",
    "\n",
    "# generate the pool\n",
    "# remove the initial data from the training dataset\n",
    "X_pool = np.delete(train_all, initial_idx, axis=0)\n",
    "y_pool = np.delete(labels, initial_idx, axis=0)\n",
    "\n",
    "# initialize ActiveLearner\n",
    "learner = ActiveLearner(\n",
    "    estimator=classifier,\n",
    "    X_training=X_initial, y_training=y_initial,\n",
    "    verbose=1,\n",
    "    validation_data=(test_all, testLabels),\n",
    "    batch_size=2048,\n",
    "    epochs=75\n",
    ")\n",
    "\n",
    "# the active learning loop\n",
    "n_queries = 10\n",
    "for idx in range(n_queries):\n",
    "    print('Query no. %d' % (idx + 1))\n",
    "    query_idx, query_instance = learner.query(X_pool, n_instances=1000, verbose=0, batch_size=1024)\n",
    "    learner.teach(\n",
    "        X=X_pool[query_idx], y=y_pool[query_idx], only_new=False,\n",
    "        verbose=1,\n",
    "        validation_data=(test_all, testLabels),\n",
    "        batch_size=1024,\n",
    "        epochs=10\n",
    "    )\n",
    "    # remove queried instance from pool\n",
    "    X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, query_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_meta_features(data):\n",
    "    # Compute emoji-based features\n",
    "    emojis = ['😂', '😭', '😞', '😢', '😁', '😅', '😍',\n",
    "              '😀', '😃', '😡', '😄', '😆', '😒', '😊',\n",
    "              '😌', '😠', '😤', '🙂', '😺', '😫', '😩',\n",
    "              '😹', '😜', '👍', '😘', '😸', '😉', '😽',\n",
    "              '😻', '😏', '💔', '😝', '😑', '🙁', '😾',\n",
    "              '😿', '😬', '❤', '😋', '🙄', '😔', '🙀',\n",
    "              '😎', '👎', '😦', '😧', '❤️', '😛', '😶',\n",
    "              '😐', '👌', '🤔','😇', '😨', '😯', '😳',\n",
    "              '☹️', '💋', '👋', '😪', '😥', '💕', '😱',\n",
    "              '🙈', '😟', '🙏', '✌', '😖', '😣', '😮',\n",
    "              '🤗', '😓', '😷', '☹', '💞', '🏻', '🙌',\n",
    "              '💐', '🙊', '😰', '☺', '😴', '🖕', '♥', '😕',\n",
    "              '😈', '💗', '♡', '👀', '👊', '‑c', ' 8‑d', ' ‑d',\n",
    "              '👻', '：）', '.', '?', '!', ',', '-', '・', \"'-'\",\n",
    "              '\\U0001f923','・ω・', '\\U000fe339', ' ‑c']\n",
    "    happy_words = ['happy', 'lol', 'haha', 'enjoy', 'cool', 'glad',\n",
    "                   'smile', 'nice', 'funny', 'wow', 'good', 'best',\n",
    "                   'party', 'baby', 'sweet', 'joke', 'glad', 'perfect',\n",
    "                   'fantastic', 'excite', 'cute', 'enjoy', 'omg']\n",
    "    angry_words = ['angry', 'fuck', 'hell', 'shut up', 'bad', 'rude',\n",
    "                  'block', 'stupid', 'piss', 'lame', \"don't\", 'mean',\n",
    "                  'irritat', 'hate', 'ignore', 'get lost', 'reply',\n",
    "                  'fool', 'regret', 'dumb', 'cheat', 'whore', 'disgust']\n",
    "    sad_words   = ['sad', 'sorry', 'miss', 'alone', 'lonely', 'cry',\n",
    "                   'disappointed', 'not', 'no', 'not happy', 'crazy',\n",
    "                   'stress', 'depress', 'poor', 'care', 'health', 'break up',\n",
    "                   'breaking up', 'upset', 'forgive', 'left me', 'dump']\n",
    "    others_words = ['thank you', 'favorite', 'favourite']\n",
    "    indicator_words = emojis + happy_words + angry_words + sad_words + others_words\n",
    "    \n",
    "    word_features = np.zeros((len(data), len(indicator_words)))\n",
    "    for i, text in enumerate(data):\n",
    "        for j, word in enumerate(indicator_words):\n",
    "            useful_text = text.lower()\n",
    "#             useful_text = \" \".join([text.split(' <eos> ')[0], text.split(' <eos> ')[-1]]).lower()\n",
    "            word_features[i][j] += useful_text.count(word)\n",
    "    \n",
    "    # Compute CAPS-based features\n",
    "    capital_features = np.zeros((len(data), 3))\n",
    "    for i, text in enumerate(data):\n",
    "        for word in text.split(' '):\n",
    "            if word.isupper():\n",
    "                capital_features[i][0] += 1\n",
    "        capital_features[i][1] = capital_features[i][0] / (len(text.split(' ')) + 1)\n",
    "        capital_features[i][2] = sum([len(x) for x in text.split(' ')]) / len(text.split(' '))\n",
    "    \n",
    "    # Combine metadata-based features\n",
    "    metadata_features = np.concatenate((word_features, capital_features), axis=1)\n",
    "    return metadata_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30160, 180)\n"
     ]
    }
   ],
   "source": [
    "metadata_features = construct_meta_features(rawtrainTexts)\n",
    "print(metadata_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [14930.  4063.  5074.  5246.]\n",
      "False Positives per class :  [787.  12.  44.   4.]\n",
      "False Negatives per class :  [ 18. 180. 389. 260.]\n",
      "Class happy : Precision : 0.997, Recall : 0.958, F1 : 0.977\n",
      "Class sad : Precision : 0.991, Recall : 0.929, F1 : 0.959\n",
      "Class angry : Precision : 0.999, Recall : 0.953, F1 : 0.975\n",
      "Ignoring the Others class, Macro Precision : 0.9959, Macro Recall : 0.9464, Macro F1 : 0.9705\n",
      "Ignoring the Others class, Micro TP : 14383, FP : 60, FN : 829\n",
      "Accuracy : 0.9719, Micro Precision : 0.9958, Micro Recall : 0.9455, Micro F1 : 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9719164456233422, 0.99584574, 0.94550353, 0.9700219082279653)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier for metadata-based classification\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "meta_clf = tree.DecisionTreeClassifier()\n",
    "meta_clf.fit(metadata_features, np.argmax(labels, axis=1))\n",
    "utils.getMetrics(meta_clf.predict_proba(metadata_features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_all, batch_size=1024)\n",
    "eval_predictions = model.predict(eval_all, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = construct_meta_features(rawtestTexts)\n",
    "predictions_meta = meta_clf.predict_proba(test_meta)\n",
    "\n",
    "eval_meta = construct_meta_features(rawevalTexts)\n",
    "eval_predictions_meta = meta_clf.predict_proba(eval_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [2202.   75.   92.  124.]\n",
      "False Positives per class :  [110.  41.  35.  76.]\n",
      "False Negatives per class :  [136.  67.  33.  26.]\n",
      "Class happy : Precision : 0.647, Recall : 0.528, F1 : 0.581\n",
      "Class sad : Precision : 0.724, Recall : 0.736, F1 : 0.730\n",
      "Class angry : Precision : 0.620, Recall : 0.827, F1 : 0.709\n",
      "Ignoring the Others class, Macro Precision : 0.6637, Macro Recall : 0.6969, Macro F1 : 0.6799\n",
      "Ignoring the Others class, Micro TP : 291, FP : 152, FN : 126\n",
      "Accuracy : 0.9049, Micro Precision : 0.6569, Micro Recall : 0.6978, Micro F1 : 0.6767\n",
      "\n",
      "True Positives per class :  [2138.  102.   94.  127.]\n",
      "False Positives per class :  [ 76. 114.  35.  69.]\n",
      "False Negatives per class :  [200.  40.  31.  23.]\n",
      "Class happy : Precision : 0.472, Recall : 0.718, F1 : 0.570\n",
      "Class sad : Precision : 0.729, Recall : 0.752, F1 : 0.740\n",
      "Class angry : Precision : 0.648, Recall : 0.847, F1 : 0.734\n",
      "Ignoring the Others class, Macro Precision : 0.6163, Macro Recall : 0.7723, Macro F1 : 0.6855\n",
      "Ignoring the Others class, Micro TP : 323, FP : 218, FN : 94\n",
      "Accuracy : 0.8933, Micro Precision : 0.5970, Micro Recall : 0.7746, Micro F1 : 0.6743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8932849364791289, 0.5970425, 0.77458036, 0.6743215332041502)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.getMetrics(predictions, testLabels)\n",
    "print()\n",
    "predictions_sly = np.copy(predictions)\n",
    "eval_predictions_sly = np.copy(eval_predictions)\n",
    "\n",
    "for i in range(len(predictions_sly)):\n",
    "    if predictions_sly[i][1] >= 0.2:\n",
    "        predictions_sly[i] = [0, 1, 0, 0]\n",
    "#     if predictions_sly[i][0] >= 0.5:\n",
    "#         predictions_sly[i] = [1, 0, 0, 0]\n",
    "#     elif predictions_sly[i][1] >= 0.4:\n",
    "#         predictions_sly[i] = [0, 1, 0, 0]\n",
    "#     elif predictions_sly[i][2] >= 0.5:\n",
    "#         predictions_sly[i] = [0, 0, 1, 0]\n",
    "  \n",
    "alpha = 0.8\n",
    "# utils.getMetrics(predictions_sly, testLabels)\n",
    "# print()\n",
    "utils.getMetrics(alpha * predictions_sly + (1-alpha) * predictions_meta, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# predictions_towrite = (predictions).argmax(axis=1)\n",
    "predictions_towrite = (alpha * predictions_sly + (1-alpha) * predictions_meta).argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[predictions_towrite[lineNum]] + '\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# predictions_towrite = (predictions).argmax(axis=1)\n",
    "# evals_towrite = (alpha * eval_predictions_sly + (1-alpha) * eval_predictions_meta).argmax(axis=1)\n",
    "evals_towrite = eval_predictions_sly.argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(evalDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[evals_towrite[lineNum]] + '\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona",
   "language": "python",
   "name": "persona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
